{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624f1b09-808e-4afd-9790-ab44ebe3a2ac",
   "metadata": {
    "id": "624f1b09-808e-4afd-9790-ab44ebe3a2ac"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6af0d-9530-4f4b-b901-486d35a30e95",
   "metadata": {
    "id": "8ed6af0d-9530-4f4b-b901-486d35a30e95"
   },
   "source": [
    "## Forward Propagation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0b5fd2-e0f2-403a-a8b3-26fd74634936",
   "metadata": {
    "id": "ce0b5fd2-e0f2-403a-a8b3-26fd74634936"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize the parameters for a multi-layer neural network.\n",
    "\n",
    "    Args:\n",
    "    layer_dims (list): List containing the dimensions of each layer in the network.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the initialized weights and biases.\n",
    "          Weights 'W' and biases 'b' for each layer l are keyed by 'Wl' and 'bl' respectively.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l - 1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        # print(f\"Layer {l}, W shape: {parameters['W' + str(l)].shape}, max W: {np.max(parameters['W' + str(l)])}, min W: {np.min(parameters['W' + str(l)])}\")  # Debugging statement\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0bf9f0-2a0c-49a1-bace-a716c6fb5835",
   "metadata": {
    "id": "de0bf9f0-2a0c-49a1-bace-a716c6fb5835"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Args:\n",
    "    A (numpy.ndarray): Activations from previous layer (or input data): shape (size of previous layer, number of examples)\n",
    "    W (numpy.ndarray): Weights matrix: shape (size of current layer, size of previous layer)\n",
    "    b (numpy.ndarray): Bias vector, shape (size of current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The linear component of the activation function for the current layer. Z = W*A + b\n",
    "    dict: A cache containing the inputs A, W, and b, to be used in backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "\n",
    "    return Z, linear_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f599f7ea-fd53-4838-bdc0-f64567e4fdd1",
   "metadata": {
    "id": "f599f7ea-fd53-4838-bdc0-f64567e4fdd1"
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute the softmax activation for a given input.\n",
    "\n",
    "    Args:\n",
    "    Z (numpy.ndarray): The linear component of the activation function from the current layer; shape (size of current layer, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The activations after applying softmax, representing probability distributions over classes.\n",
    "    numpy.ndarray: Returns Z, cached for use in backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    A = expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84499ba-63f5-42ac-a29e-365d1ed95c84",
   "metadata": {
    "id": "a84499ba-63f5-42ac-a29e-365d1ed95c84"
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    compute ReLU activation for a givem input.\n",
    "\n",
    "    Args:\n",
    "    Z (numpy,ndarray): The linear component of the activation function\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The activations after applying ReLU, where each element is the max of 0 and the element itself.\n",
    "    numpy.ndarray: Returns Z, cached for use in backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0.0, Z)\n",
    "\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84053518-cd44-4731-b0cd-ad48c41a6518",
   "metadata": {
    "id": "84053518-cd44-4731-b0cd-ad48c41a6518"
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Args:\n",
    "    A_prev (numpy.ndarray): Activations from previous layer.\n",
    "    W (numpy.ndarray): Weights matrix.\n",
    "    B (numpy.ndarray): Bias vector.\n",
    "    activation (str): The type of activation function to be used (\"relu\" or \"softmax\").\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The activations of the current layer.\n",
    "    dict: A cache containing both linear and activation caches for use in backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66959df9-50ab-4b1f-bde8-7b0a14823ae4",
   "metadata": {
    "id": "66959df9-50ab-4b1f-bde8-7b0a14823ae4"
   },
   "source": [
    "#### Batch-Normalization (BN)\n",
    "Consists of normalizing activation vectors from hidden layers using the first and the second statistical moments (mean and variance) of the current batch.\n",
    "This normalization step is applied right before (or right after) the nonlinear function.\n",
    "It is usually used as a module which could be inserted as a standard layer in a DNN.\n",
    "\n",
    "The BN layer first determines the mean 𝜇 and the variance σ² of the activation values across the batch, using (1) and (2).\n",
    "\n",
    "It then normalizes the activation vector Z^(i) with (3). That way, each neuron’s output follows a standard normal distribution across the batch. (𝜀 is a constant used for numerical stability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea06367-e5c9-4011-92d5-37ccf1c97208",
   "metadata": {
    "id": "bea06367-e5c9-4011-92d5-37ccf1c97208"
   },
   "outputs": [],
   "source": [
    "def apply_batchnorm(Z):\n",
    "    \"\"\"\n",
    "    Apply batch normalization to the linear output Z.\n",
    "\n",
    "    Args:\n",
    "    A (numpy.ndarray): Input 2D array where each row corresponds to a feature\n",
    "                       and each column corresponds to an example.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The normalized 2D array where each feature (row) has a mean\n",
    "                   of 0 and a variance of 1.\n",
    "    \"\"\" \n",
    "    mean = np.mean(Z, axis=1, keepdims=True)\n",
    "    var = np.var(Z, axis=1, keepdims=True)\n",
    "    NA = (Z - mean) / np.sqrt(var + 1e-8)\n",
    "    return NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f24229c-cbdf-43fb-a70e-a80a43b31e18",
   "metadata": {
    "id": "7f24229c-cbdf-43fb-a70e-a80a43b31e18"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, use_batchnorm=False):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation.\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): Input data of shape (input size, number of examples).\n",
    "    parameters (dict): Python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\".\n",
    "                       - Wl: weight matrix of shape (size of previous layer, size of current layer)\n",
    "                       - bl: bias vector of shape (1, size of current layer)\n",
    "    use_batchnorm (bool): If True, apply batch normalization after each linear transformation.\n",
    "\n",
    "    Returns:\n",
    "    AL (numpy.ndarray): Last post-activation value (output of the model).\n",
    "    caches (list): List of caches containing:\n",
    "                   - every cache of linear_activation_forward() (for relu) \n",
    "                   - the cache of linear_activation_forward() (for softmax)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # Number of layers\n",
    "\n",
    "    # Forward pass for [LINEAR->RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters['W' + str(l)]\n",
    "        B = parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, B, \"relu\")\n",
    "        if use_batchnorm:\n",
    "            Z = cache[1]  # activation_cache is second in tuple, Z is stored in it\n",
    "            Z_norm = apply_batchnorm(Z)\n",
    "            A, _ = relu(Z_norm)  # Reapply activation after batch normalization\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Forward pass for the last layer [LINEAR->SOFTMAX]\n",
    "    W = parameters['W' + str(L)]\n",
    "    B = parameters['b' + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, B, \"softmax\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "878649e5-1a6e-4c3c-b9ae-7b3b9539fae6",
   "metadata": {
    "id": "878649e5-1a6e-4c3c-b9ae-7b3b9539fae6"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, parameters ,epsilon_L2=0):\n",
    "    \"\"\"\n",
    "    Compute the categorical cross-entropy cost.\n",
    "\n",
    "    Args:\n",
    "    AL (numpy.ndarray): Probability vector corresponding to label predictions, shape (num_of_classes, number of examples)\n",
    "    Y (numpy.ndarray): Ground truth labels vector, shape (num_of_classes, number of examples)\n",
    "    parameters:\n",
    "    epsilon_L2 (float): L2 Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "    float: The cross-entropy cost.\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    # cost = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)) / m\n",
    "    cost = -np.sum(Y * np.log(AL + 1e-8)) / m  # Categorical cross-entropy cost\n",
    "    cost = np.squeeze(cost)  # To make sure cost is a scalar\n",
    "\n",
    "    #add L2 Norm Regularization\n",
    "    L2_cost = 0\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        L2_cost += np.sum(np.square(parameters[\"W\" + str(l)]))\n",
    "    L2_cost = (epsilon_L2 / (2 * m)) * L2_cost\n",
    "\n",
    "    return cost + L2_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e9171-7c24-410d-b3d8-854cf6594b26",
   "metadata": {
    "id": "539e9171-7c24-410d-b3d8-854cf6594b26"
   },
   "source": [
    "## Backward Propagation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a1932f-26d5-4dba-9314-22585839b1da",
   "metadata": {
    "id": "86a1932f-26d5-4dba-9314-22585839b1da"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer.\n",
    "\n",
    "    Args:\n",
    "    dZ (numpy.ndarray): Gradient of the cost with respect to the linear output of the current layer l\n",
    "    cache (tuple): Tuple of values (A_prev, W, b) from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    dA_prev (numpy.ndarray): Gradient of the cost with respect to the activation of the previous layer\n",
    "    dW (numpy.ndarray): Gradient of the cost with respect to W (current layer l)\n",
    "    db (numpy.ndarray): Gradient of the cost with respect to b (current layer l)\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe89f70-282d-47db-898b-5520e02da9cd",
   "metadata": {
    "id": "abe89f70-282d-47db-898b-5520e02da9cd"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    The backward propagation for a single RELU unit.\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9894e5-0f16-49a0-9abe-f03fddd3c6af",
   "metadata": {
    "id": "9c9894e5-0f16-49a0-9abe-f03fddd3c6af"
   },
   "outputs": [],
   "source": [
    "def softmax_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single SOFTMAX unit.\n",
    "\n",
    "    The derivative of the softmax function is: p_i - y_i,\n",
    "    where p_i is the softmax-adjusted probability of the class and y_i is the “ground truth”.\n",
    "\n",
    "    Args:\n",
    "    dA (numpy.ndarray): The post-activation gradient, typically AL - Y.\n",
    "    activation_cache (numpy.ndarray): Contains Z (stored during the forward propagation).\n",
    "\n",
    "    Returns:\n",
    "    dZ (numpy.ndarray): Gradient of the cost with respect to Z.\n",
    "    \"\"\"\n",
    "\n",
    "    # The gradient of the cost with respect to Z for softmax is simply the difference\n",
    "    # between the predicted probabilities (AL) and the true labels (Y), which is dA.\n",
    "    # In the context of backpropagation, dA is typically computed as AL - Y.\n",
    "    # Therefore, we return dA directly as dZ.\n",
    "    return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00113cf1-2d20-48ac-a4b2-f453f0072aba",
   "metadata": {
    "id": "00113cf1-2d20-48ac-a4b2-f453f0072aba"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Args:\n",
    "    dA (numpy array): post-activation gradient for current layer\n",
    "    cache (tuple): contains the linear_cache and activation_cache (linear_cache, activation_cache)\n",
    "    activation (str): the activation to be used in this layer, stored as a text string: \"relu\", \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev (numpy array): Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW (numpy array): Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db (numpy array): Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6b7d40-0f2d-4c29-83fe-e892b9e6e425",
   "metadata": {
    "id": "2b6b7d40-0f2d-4c29-83fe-e892b9e6e425"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the entire network.\n",
    "\n",
    "    Args:\n",
    "    AL (numpy.ndarray): Probability vector, output of the forward propagation (L_model_forward).\n",
    "    Y (numpy.ndarray): True \"label\" vector (the same shape as AL).\n",
    "    caches (list): List of caches containing:\n",
    "                   - every cache of linear_activation_forward() with \"relu\"\n",
    "                   - the cache of linear_activation_forward() with \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    grads (dict): A dictionary with the gradients:\n",
    "                  - \"dA1\", \"dA2\", ..., \"dAL\"\n",
    "                  - \"dW1\", \"dW2\", ..., \"dWL\"\n",
    "                  - \"db1\", \"db2\", ..., \"dbL\"\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    # dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    # dAL = - (np.divide(Y, AL + 1e-8))\n",
    "    dAL = AL - Y\n",
    "\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "\n",
    "    # loop from L-2 to 0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "954571da-2829-40be-a0d3-7afb035cf98e",
   "metadata": {
    "id": "954571da-2829-40be-a0d3-7afb035cf98e"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate, epsilon_L2):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "\n",
    "    Args:\n",
    "    parameters (dict): A dictionary containing the DNN architecture's parameters\n",
    "    grads (dict): A dictionary containing the gradients (generated by L_model_backward)\n",
    "    learning_rate (float): The learning rate, alpha, used to update the parameters\n",
    "    epsilon_L2 (float): L2 Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "    dict: The updated values of the parameters\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(1, L+1):\n",
    "        m = parameters[\"W\" + str(l)].shape[1]\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * (grads[\"dW\" + str(l)] +\n",
    "                                               epsilon_L2 / m * parameters[\"W\" + str(l)])\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f07ddb-4dd9-4d37-a274-eb08c9c82bc6",
   "metadata": {
    "id": "02f07ddb-4dd9-4d37-a274-eb08c9c82bc6"
   },
   "source": [
    "## Train the Network and Produce Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d205b845-6172-4c06-a9f2-584ffd1206aa",
   "metadata": {
    "id": "d205b845-6172-4c06-a9f2-584ffd1206aa"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    function to split X and Y to mini batches of size mini_batch_size\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): Input data, shape (height*width, number_of_examples).\n",
    "    Y (numpy.ndarray): True labels, shape (num_of_classes, number of examples).\n",
    "    mini_batch_size(int): size of the mini-batches\n",
    "\n",
    "    Returns:\n",
    "    mini_batches: array of mini batches\n",
    "    \"\"\"\n",
    "    # np.random.seed(3)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da018826-965d-49cb-9b86-676dd4b6f673",
   "metadata": {
    "id": "da018826-965d-49cb-9b86-676dd4b6f673"
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate , num_iterations, batch_size, use_batchnorm=False, epsilon_L2=0):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network.\n",
    "    All layers but the last should have the ReLU activation function,\n",
    "    and the final layer will apply the softmax activation function.\n",
    "    The size of the output layer should be equal to the number of labels in the data.\n",
    "    Please select a batch size that enables your code to run well\n",
    "    (i.e. no memory overflows while still running relatively fast).\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        minibatches = random_mini_batches(X, Y, batch_size) #TODO: check if neccecry\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            AL, caches = L_model_forward(minibatch_X, parameters, use_batchnorm=use_batchnorm)\n",
    "            cost = compute_cost(AL, minibatch_Y, parameters, epsilon_L2)\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches)\n",
    "            parameters = update_parameters(parameters, grads, learning_rate, epsilon_L2)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b6ee5df-48ee-4f14-b48c-1915f3e365fe",
   "metadata": {
    "id": "1b6ee5df-48ee-4f14-b48c-1915f3e365fe"
   },
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Predicts the results using a deep learning model and calculates the accuracy.\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): Input data, shape (height*width, number_of_examples).\n",
    "    Y (numpy.ndarray): True labels, shape (num_of_classes, number of examples).\n",
    "    parameters (dict): Trained parameters of the DNN.\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    AL, _ = L_model_forward(X, parameters)\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    true_labels = np.argmax(Y, axis=0)\n",
    "    accuracy = np.mean(predictions == true_labels) * 100\n",
    "    # return predictions, accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b37dd-2f99-4b1b-a7f2-b0722851906b",
   "metadata": {
    "id": "0b7b37dd-2f99-4b1b-a7f2-b0722851906b"
   },
   "source": [
    "## classify the MNIST dataset and present a summary report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K0R6HIE2ybFk",
   "metadata": {
    "id": "K0R6HIE2ybFk"
   },
   "source": [
    "### Prepare mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Y0GUTdcFCNdZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0GUTdcFCNdZ",
    "outputId": "b5c02e2d-3c3d-4590-b2a3-4aefb4301277"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 12:35:14.606842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 12:35:16.391587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (784, 48000)\n",
      "Validation set shape: (784, 12000)\n",
      "Test set shape: (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train_full, Y_train_full), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten the data\n",
    "X_train_full = X_train_full.reshape(X_train_full.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_train_full = tf.keras.utils.to_categorical(Y_train_full, 10).T\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, 10).T\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_full.T, Y_train_full.T, test_size=0.2, random_state=42)\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "Y_train = Y_train.T\n",
    "Y_val = Y_val.T\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f2814bb-dd64-4dff-82d0-bdbce46c5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "def train_model_on_mnist(X_train, Y_train, X_val, Y_val, use_batchnorm=False, epsilon_L2=0, patience=100, improvement_threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Function to train a DNN model on the MNIST dataset.\n",
    "    Train the network until there is no improvement on the validation set\n",
    "    (or the improvement is very small) for a specified number of training steps.\n",
    "\n",
    "    Args:\n",
    "    X_train (numpy.ndarray): Training data.\n",
    "    Y_train (numpy.ndarray): Training labels.\n",
    "    X_val (numpy.ndarray): Validation data.\n",
    "    Y_val (numpy.ndarray): Validation labels.\n",
    "    use_batchnorm (boolean): Flag to use batch normalization.\n",
    "    epsilon_L2 (float): L2 Regularization parameter.\n",
    "    patience (int): Number of training steps to wait for an improvement before stopping.\n",
    "    improvement_threshold (float): Minimum improvement to consider.\n",
    "\n",
    "    Returns:\n",
    "    parameters (dict): Trained model parameters.\n",
    "    training_steps (int): Number of training steps performed.\n",
    "    epochs (int): Number of epochs performed.\n",
    "    \"\"\"\n",
    "    # Define the network architecture\n",
    "    layers_dims = [X_train.shape[0], 20, 7, 5, 10]\n",
    "    learning_rate = 0.009\n",
    "    batch_size = 64\n",
    "\n",
    "    best_val_cost = float('inf')\n",
    "    training_steps = 0\n",
    "    epochs_without_improvement = 0\n",
    "    total_iterations = 0\n",
    "\n",
    "    iterations_per_epoch = X_train.shape[1] // batch_size\n",
    "\n",
    "    while epochs_without_improvement < patience:\n",
    "        # Train the model for 100 iterations\n",
    "        parameters, costs = L_layer_model(X_train, Y_train, layers_dims, learning_rate=learning_rate,\n",
    "                                          num_iterations=100, batch_size=batch_size, use_batchnorm=use_batchnorm, epsilon_L2=epsilon_L2)\n",
    "        training_steps += 100\n",
    "        total_iterations += 100\n",
    "\n",
    "        # Compute validation loss\n",
    "        AL_val, _ = L_model_forward(X_val, parameters, use_batchnorm=use_batchnorm)\n",
    "        val_cost = compute_cost(AL_val, Y_val, parameters, epsilon_L2=epsilon_L2)\n",
    "\n",
    "        if best_val_cost - val_cost > improvement_threshold:\n",
    "            best_val_cost = val_cost\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 100\n",
    "\n",
    "        print(f\"Training steps: {training_steps}, Validation cost: {val_cost:.4f}, Best validation cost: {best_val_cost:.4f}\")\n",
    "\n",
    "    # Calculate the number of epochs based on total iterations and iterations per epoch\n",
    "    total_epochs = total_iterations / iterations_per_epoch\n",
    "\n",
    "    return parameters, training_steps, total_epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LOBxv9f_ymSn",
   "metadata": {
    "id": "LOBxv9f_ymSn"
   },
   "source": [
    "### Without batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0ee9845-e55a-42e5-849f-52c0a584fbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.7001067863708594\n",
      "Training steps: 100, Validation cost: 0.2983, Best validation cost: 0.2983\n",
      "Cost after iteration 0: 1.775134069081191\n",
      "Training steps: 200, Validation cost: 0.2530, Best validation cost: 0.2530\n",
      "Cost after iteration 0: 1.5937900178811995\n",
      "Training steps: 300, Validation cost: 0.2244, Best validation cost: 0.2244\n",
      "Cost after iteration 0: 1.1332600397598123\n",
      "Training steps: 400, Validation cost: 0.2440, Best validation cost: 0.2244\n",
      "Train accuracy: 97.02%\n",
      "Test accuracy: 94.31%\n",
      "Validation accuracy: 93.92%\n",
      "Total iterations: 400\n",
      "Total epochs: 0.53\n",
      "Training time: 164.41 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "parameters, training_steps, total_epochs = train_model_on_mnist(X_train, Y_train, X_val, Y_val, use_batchnorm=False, epsilon_L2=0)\n",
    "end_time = perf_counter()\n",
    "\n",
    "time_without_batchnorm = end_time - start_time\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracy = predict(X_train, Y_train, parameters)\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = predict(X_test, Y_test, parameters)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy = predict(X_val, Y_val, parameters)\n",
    "print(f\"Validation accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Total iterations: {training_steps}\")\n",
    "print(f\"Total epochs: {total_epochs:.2f}\")\n",
    "print(f\"Training time: {time_without_batchnorm:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb63f78-6c83-4a1c-8677-b054c3317813",
   "metadata": {
    "id": "9cb63f78-6c83-4a1c-8677-b054c3317813"
   },
   "source": [
    "### With batchnorm\n",
    "classify the MNIST dataset and present a summary report (batchnorm function is “on”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b907e3b-db3e-460a-a5df-e218c9e8e0cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b907e3b-db3e-460a-a5df-e218c9e8e0cc",
    "outputId": "3786e099-08c2-4eb2-c1c3-6480767480fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.9186942963148828\n",
      "Training steps: 100, Validation cost: 1.5564, Best validation cost: 1.5564\n",
      "Cost after iteration 0: 2.065306905636067\n",
      "Training steps: 200, Validation cost: 1.8803, Best validation cost: 1.5564\n",
      "Train accuracy: 13.63%\n",
      "Test accuracy: 13.47%\n",
      "Validation accuracy: 13.41%\n",
      "Total iterations: 200\n",
      "Total epochs: 0.27\n",
      "Training time: 108.77 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "parameters_batchnorm, training_steps_batchnorm, total_epochs_batchnorm = train_model_on_mnist(X_train, Y_train, X_val, Y_val, use_batchnorm=True, epsilon_L2=0)\n",
    "end_time = perf_counter()\n",
    "\n",
    "time_with_batchnorm = end_time - start_time\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracy_batchnorm = predict(X_train, Y_train, parameters_batchnorm)\n",
    "print(f\"Train accuracy: {train_accuracy_batchnorm:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy_batchnorm = predict(X_test, Y_test, parameters_batchnorm)\n",
    "print(f\"Test accuracy: {test_accuracy_batchnorm:.2f}%\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy_batchnorm = predict(X_val, Y_val, parameters_batchnorm)\n",
    "print(f\"Validation accuracy: {val_accuracy_batchnorm:.2f}%\")\n",
    "\n",
    "print(f\"Total iterations: {training_steps_batchnorm}\")\n",
    "print(f\"Total epochs: {total_epochs_batchnorm:.2f}\")\n",
    "print(f\"Training time: {time_with_batchnorm:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9WTyy_Vo0ysC",
   "metadata": {
    "id": "9WTyy_Vo0ysC"
   },
   "source": [
    "compare this experiment to the previous one (performance, running time, number of training steps etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "nW-Ea5jo03me",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW-Ea5jo03me",
    "outputId": "5f014e8f-a125-443f-fa94-6396427b1962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: \n",
      "\twithout batchnorm:  164.4076 sec\n",
      "\twith batchnorm:  108.7736 sec\n",
      "Test accuracy: \n",
      "\twithout batchnorm:  94.3100%\n",
      "\twith batchnorm:  13.4700%\n",
      "Train accuracy: \n",
      "\twithout batchnorm:  97.0208%\n",
      "\twith batchnorm:  13.6250%\n",
      "Validate accuracy: \n",
      "\twithout batchnorm:  93.9167%\n",
      "\twith batchnorm:  13.4083%\n",
      "Training steps: \n",
      "\twithout batchnorm:  400.0\n",
      "\twith batchnorm:  200.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running time: \")\n",
    "print(f\"\\twithout batchnorm: {time_without_batchnorm: .4f} sec\")\n",
    "print(f\"\\twith batchnorm: {time_with_batchnorm: .4f} sec\")\n",
    "\n",
    "print(f\"Test accuracy: \")\n",
    "print(f\"\\twithout batchnorm: {test_accuracy: .4f}%\")\n",
    "print(f\"\\twith batchnorm: {test_accuracy_batchnorm: .4f}%\")\n",
    "\n",
    "print(f\"Train accuracy: \")\n",
    "print(f\"\\twithout batchnorm: {train_accuracy: .4f}%\")\n",
    "print(f\"\\twith batchnorm: {train_accuracy_batchnorm: .4f}%\")\n",
    "\n",
    "print(f\"Validate accuracy: \")\n",
    "print(f\"\\twithout batchnorm: {val_accuracy: .4f}%\")\n",
    "print(f\"\\twith batchnorm: {val_accuracy_batchnorm: .4f}%\")\n",
    "\n",
    "print(f\"Training steps: \")\n",
    "print(f\"\\twithout batchnorm: {training_steps: .1f}\")\n",
    "print(f\"\\twith batchnorm: {training_steps_batchnorm: .1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5713428-0c8a-4360-9363-adfbf8834726",
   "metadata": {
    "id": "b5713428-0c8a-4360-9363-adfbf8834726"
   },
   "source": [
    "### Modify the code to support the L2 standard functionality\n",
    "\n",
    "Changes in the code:\n",
    "\n",
    "\n",
    "*   compute_cost: add parameters: epsilon_L2 and the dict 'parameters'. Then calulate the L2 cost and add to the cros-entropy cost.\n",
    "*   update_parameters: add parameter epsilon_L2 and update in the function the equation of updates W with the L2 Regularization\n",
    "*   L_layer_model: add parameter epsilon_L2 to the signature and update the call to compute_cost with the new parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59496753-18f0-4f22-9fa2-341870c5254b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59496753-18f0-4f22-9fa2-341870c5254b",
    "outputId": "24b4073a-31a1-4be7-f0a5-222a7348e8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.8162497788469927\n",
      "Training steps: 100, Validation cost: 1.6965, Best validation cost: 1.6965\n",
      "Cost after iteration 0: 2.043473990150488\n",
      "Training steps: 200, Validation cost: 1.5872, Best validation cost: 1.5872\n",
      "Cost after iteration 0: 2.002569312064333\n",
      "Training steps: 300, Validation cost: 1.7081, Best validation cost: 1.5872\n",
      "Train accuracy: 11.62%\n",
      "Test accuracy: 11.67%\n",
      "Validation accuracy: 11.57%\n",
      "Total iterations: 300\n",
      "Total epochs: 0.40\n",
      "Training time: 163.10 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "parameters_L2, training_steps_L2, total_epochs_L2 = train_model_on_mnist(X_train, Y_train, X_val, Y_val, use_batchnorm=True, epsilon_L2=0)\n",
    "end_time = perf_counter()\n",
    "\n",
    "time_with_L2 = end_time - start_time\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracy_L2 = predict(X_train, Y_train, parameters_L2)\n",
    "print(f\"Train accuracy: {train_accuracy_L2:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy_L2 = predict(X_test, Y_test, parameters_L2)\n",
    "print(f\"Test accuracy: {test_accuracy_L2:.2f}%\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy_L2 = predict(X_val, Y_val, parameters_L2)\n",
    "print(f\"Validation accuracy: {val_accuracy_L2:.2f}%\")\n",
    "\n",
    "print(f\"Total iterations: {training_steps_L2}\")\n",
    "print(f\"Total epochs: {total_epochs_L2:.2f}\")\n",
    "print(f\"Training time: {time_with_L2:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "SPkuMd1bwWzA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPkuMd1bwWzA",
    "outputId": "c9c59929-00ad-4479-b8d7-c26248000f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: \n",
      "\twithout L2 norm :  164.4076 sec\n",
      "\twith L2 norm:  163.0952 sec\n",
      "Test accuracy: \n",
      "\twithout L2 norm:  94.3100%\n",
      "\twith L2 norm:  11.6700%\n",
      "Train accuracy: \n",
      "\twithout L2 norm:  97.0208%\n",
      "\twith L2 norm:  11.6208%\n",
      "Validate accuracy: \n",
      "\twithout L2 norm:  93.9167%\n",
      "\twith L2 norm:  11.5667%\n",
      "Training steps: \n",
      "\twithout L2 norm:  400.0\n",
      "\twith L2 norm:  300.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running time: \")\n",
    "print(f\"\\twithout L2 norm : {time_without_batchnorm: .4f} sec\")\n",
    "print(f\"\\twith L2 norm: {time_with_L2: .4f} sec\")\n",
    "\n",
    "print(f\"Test accuracy: \")\n",
    "print(f\"\\twithout L2 norm: {test_accuracy: .4f}%\")\n",
    "print(f\"\\twith L2 norm: {test_accuracy_L2: .4f}%\")\n",
    "\n",
    "print(f\"Train accuracy: \")\n",
    "print(f\"\\twithout L2 norm: {train_accuracy: .4f}%\")\n",
    "print(f\"\\twith L2 norm: {train_accuracy_L2: .4f}%\")\n",
    "\n",
    "print(f\"Validate accuracy: \")\n",
    "print(f\"\\twithout L2 norm: {val_accuracy: .4f}%\")\n",
    "print(f\"\\twith L2 norm: {val_accuracy_L2: .4f}%\")\n",
    "\n",
    "print(f\"Training steps: \")\n",
    "print(f\"\\twithout L2 norm: {training_steps: .1f}\")\n",
    "print(f\"\\twith L2 norm: {training_steps_L2: .1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "NXDmj-BFzDWt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXDmj-BFzDWt",
    "outputId": "323c4bb7-56f2-4351-b12d-2a7a48e9cf07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "\tFrobenius norm without regularization: 9.416273058342117\n",
      "\tFrobenius norm with regularization: 35486324.266033225\n",
      "\n",
      "Layer 2:\n",
      "\tFrobenius norm without regularization: 6.757597343145094\n",
      "\tFrobenius norm with regularization: 5818.79530094917\n",
      "\n",
      "Layer 3:\n",
      "\tFrobenius norm without regularization: 6.340671153811876\n",
      "\tFrobenius norm with regularization: 69.33215855108277\n",
      "\n",
      "Layer 4:\n",
      "\tFrobenius norm without regularization: 6.9658014538908635\n",
      "\tFrobenius norm with regularization: 9.691750845831352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_frobenius_norm(parameters):\n",
    "    \"\"\"\n",
    "    Compute the Frobenius norm of the weights in the parameter dictionary.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "\n",
    "    Returns:\n",
    "    norms -- list of Frobenius norms of the weight matrices\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    norms = []\n",
    "    for l in range(1, L + 1):\n",
    "        norm = np.linalg.norm(parameters['W' + str(l)], 'fro')\n",
    "        norms.append(norm)\n",
    "    return norms\n",
    "\n",
    "# Compute the Frobenius norms of the weights for both models\n",
    "norms_no_L2 = compute_frobenius_norm(parameters)\n",
    "norms_L2 = compute_frobenius_norm(parameters_L2)\n",
    "\n",
    "# Print the comparison of the norms\n",
    "for l in range(len(norms_no_L2)):\n",
    "    print(f\"Layer {l+1}:\")\n",
    "    print(f\"\\tFrobenius norm without regularization: {norms_no_L2[l]}\")\n",
    "    print(f\"\\tFrobenius norm with regularization: {norms_L2[l]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TSAndRSc05PW",
   "metadata": {
    "id": "TSAndRSc05PW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127ba37-29cc-4005-8826-b59b5124fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
